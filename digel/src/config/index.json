{
  "company": {
    "name": "Digel",
    "logo": "/assets/images/logo-light.png"
  },
  "navigation": [
    {
      "name": "Introduction",
      "href": "introduction"
    },
    {
      "name": "Customers",
      "href": "customers"
    },
    {
      "name": "Employees",
      "href": "employees"
    },
    {
      "name": "Latest News",
      "href": "latest-news"
    },
    {
      "name": "About",
      "href": "about"
    }
  ],
  "callToAction": {
    "text": "Get started",
    "href": "#"
  },
  "title": {
    "slogan": "Delivering Industrial Digitalization with Cybernetics Expertise",
    "img": "/assets/images/logo-light.png"
  },
  "introduction": {
    "title": "",
    "items": [
      {
        "title": "About Us",
        "content": [
          "Our team includes engineers with strong backgrounds in cybernetics, robotics, and industrial IT. We have solid expertise in automation systems, covering both Operational Technology (OT) and Information Technology (IT) aspects. Whether it's developing embedded software, designing control systems, working on cloud solutions, or managing frontend development, we can handle it. We also excel in managing large amounts of timeseries data and applying machine learning and AI technologies."
        ],
        "img": ""
      },
      {
        "title": "Project Experience",
        "content": [
          "Our team's wide-ranging industry experience equips us to deliver innovative solutions in various sectors. Examples include: Maritime Operations, Oil & Gas, Aquaculture, Construction, Carbon Capture, Batteries, Shipyards, Electronics, Semiconductors, Agriculture, Wastewater Treatment, Fertilizer Production, and Seaweed Farming."
        ],
        "bold": [
          "Oil",
          "&",
          "Gas",
          "Maritime",
          "Operations",
          "Aquaculture",
          "Construction",
          "Carbon",
          "Capture",
          "Batteries",
          "Shipyards",
          "Electronics",
          "Semiconductors",
          "Agriculture",
          "Wastewater",
          "Treatment",
          "Fertilizer",
          "Production",
          "Seaweed",
          "Farming"
        ],
        "img": ""
      }
    ]
  },
  "customers": {
    "title": "Our Customers and Partners",
    "items": [
      {
        "name": "Aker Solutions",
        "img": "/assets/images/akerSolutions.png",
        "href": "https://www.akersolutions.com/"
      },
      {
        "name": "Hoff",
        "img": "/assets/images/hoff.png",
        "href": "https://www.hoff.no/"
      },
      {
        "name": "RunwayFBU",
        "img": "/assets/images/runwayFBU.svg",
        "href": "https://runwayfbu.com/"
      }
    ]
  },
  "employees": {
    "title": "Meet the Team!",
    "items": [
      {
        "name": "Christoffer Lange | CEO",
        "role": "",
        "img": "/assets/images/christoffer-2.png",
        "description": "M.Sc. Engineering & ICT, Norwegian University of Science and Technology",
        "keywords": ["Tech Lead", "Engineerng Automation", "Industrial IoT"]
      },
      {
        "name": "Thomas Andersen",
        "role": "",
        "img": "/assets/images/thomas-2.png",
        "description": "M.Sc. Cybernetics & Robotics, Norwegian University of Science and Technology",
        "keywords": ["Cloud", "Embedded", "Machine Learning"]
      },
      {
        "name": "Axel Bech",
        "role": "",
        "img": "/assets/images/axel-2.png",
        "description": "M.Sc. Cybernetics & Robotics, Norwegian University of Science and Technology",
        "keywords": ["Fullstack", "Modeling", "Optimization"]
      },
      {
        "name": "Vegard Sanden",
        "role": "",
        "img": "/assets/images/vegard.png",
        "description": "M.Sc. Cybernetics & Robotics, Norwegian University of Science and Technology",
        "keywords": ["Fullstack", "Modeling", "Cloud"]
      },
      {
        "name": "Erlend Blomseth",
        "role": "",
        "img": "/assets/images/erlend-2.png",
        "description": "M.Sc. Cybernetics & Robotics, Norwegian University of Science and Technology",
        "keywords": ["Fullstack", "Architect"]
      },
      {
        "name": "Bj√∏rn-Erik Dale | Chairman",
        "role": "",
        "img": "/assets/images/bjorn-erik.png",
        "description": "M.Sc. Industrial Economics & Technology Management, Norwegian University of Science and Technology",
        "keywords": []
      }
    ]
  },
  "blog": {
    "title": "Latest News",
    "posts": [
      {
        "title": "Streamlining Engineering at Aker Solutions",
        "img": "/assets/images/chris-axel.jpg",
        "author": {
          "name": "Christoffer Lange",
          "src": "/assets/images/christoffer-2.png"
        },
        "published": "4 min read - Apr 3, 2024",
        "sections": [
          {
            "content": "Digel is working with Aker Solutions to advance engineering processes, particularly in the design of stair towers for offshore installations."
          },
          {
            "title": "Project Overview",
            "content": "The application reduces the traditional design time for stair towers from several weeks to a few minutes. Using Python and the ParaPy framework, it automates the complex design process, while ensuring each stair tower is structurally sound and ergonomically efficient for safe use by offshore personnel. Additionally, the application integrates seamlessly with E3D from Aveva, a 3D design software used extensively in process and power plant engineering."
          },
          {
            "title": "Digel's Role",
            "content": "Our contribution includes software development and project management. Digel utilizes our industrial expertise and knowledge of both traditional engineering and software competencies to effectively address specific engineering challenges.",
            "img": {
              "src": "/assets/images/axel-christoffer-aker-2.jpg",
              "caption": "'It is inherent in Digel's nature to engage in processes that drive disruptive changes in industrial operations', says Christoffer, reflecting on the core philosophy that guides our work."
            }
          },
          {
            "title": "Conclusion",
            "content": "The collaborative efforts between Digel and Aker Solutions demonstrate the benefits of applying modern software solutions to streamline complex engineering tasks, setting new standards for efficiency and innovation in the industry. To achieve truly disruptive changes, it is essential to possess expertise not only in engineering but also in IT. Mastery of both disciplines is key to realizing the real benefits of digitalization."
          }
        ]
      },
      {
        "title": "An Introduction to Reinforcement Learning: What is it, and what can it be used for?",
        "author": {
          "name": "Thomas Andersen",
          "src": "/assets/images/thomas-2.png"
        },
        "relevantPost": {
          "name": "Norsk versjon",
          "url": "/blog/2"
        },
        "img": "/assets/images/rl-intro-blog-post.webp",
        "published": "12 min read - May 5, 2024",
        "sections": [
          {
            "content": "Are you curious about how machines can learn by experience and improve over time? At Digel, we believe that Reinforcement Learning (RL) can be the key framework for optimizing industrial processes. In this blog post, I will give you a simple introduction to RL, how it works, what separates it from other machine learning methods, and some use cases."
          },
          {
            "title": "The Three Classes of Machine Learning",
            "content": "Within the world of machine learning (ML), we typically categorize methods of learning into three classes: supervised learning, unsupervised learning, and RL."
          },
          {
            "content": "In supervised learning, a dataset that maps inputs to correct outputs is used for training the AI agent. Supervised learning is often used for image classification, e.g., classifying animals in pictures. The dataset used to train the animal-classifying agent will typically consist of images of a broad range of species, each individually labeled with the species name. This is why we call it supervised learning since the learning process is guided by a 'supervisor' that provides correct answers for each example in the training data. The AI agent learns to associate input images with the correct labels through this process, improving its accuracy over time."
          },
          {
            "content": "On the other hand, unsupervised learning is a set of methods that try to identify patterns and relationships within the data. A typical unsupervised learning approach is clustering techniques, i.e., grouping similar data points together. If we continue with our dataset of animal species, we may reformulate the classification problem to an unsupervised learning problem where we find similarities between species from the images without prior knowledge of the animal names."
          },
          {
            "content": "With RL, the AI agent learns by interacting with its environment. The agent receives feedback from the environment in the form of rewards or penalties based on its actions. Through trial and error, the agent aims to maximize its rewards over time (and minimize penalties) to find an optimal action to take at any given state. Continuing with our animal-themed examples, we might want to try to train an RL agent to survive in nature. The agent will receive a reward for every encounter with an animal where the agent stays alive and will receive a penalty for encounters with predators that eat the agent. By simulating enough encounters with different species, the agent will learn to avoid a set of species.",
            "img": {
              "src": "/assets/images/rl-intro-architecture-blog-post.jpg",
              "caption": "The basic components of all reinforcement learning architectures consists of an agent interacting with an environment."
            }
          },
          {
            "title": "RL vs. Control Theory",
            "content": "As exemplified in the last paragraph, RL agents are trained to make sequences of actions that maximize some notion of cumulative reward. In reinforcement learning literature, one often refers to the decision-making function as the agent's policy. The goal is to optimize the policy in such a way that maximizes the agent's reward when interacting with the environment. In other words, the problems solved by RL may also be phrased as control problems. The policy that is continuously updated for each simulation cycle may be viewed as the control law used for making optimized actions in the environment. The main difference between traditional control theory and reinforcement learning is that RL involves adaptive learning from interactions, while control theory usually involves pre-designed controllers based on system models. It is due to reinforcement learning's trial-and-error approach to finding optimal policies that reinforcement learning excels in complex, uncertain, dynamic environments where learning from experience is crucial."
          },
          {
            "title": "Where is RL used today?",
            "content": "Reinforcement learning is for instance used in e-commerce to enhance suggestion systems. By analyzing user behavior and feedback, RL algorithms can learn to suggest products that are more likely to interest the user, thereby improving user experience and increasing sales. For example, an RL-based recommendation system might suggest new products based on past purchases and browsing history, continually refining its suggestions as it learns more about user preferences."
          },
          {
            "content": "In the gaming industry, reinforcement learning has led to significant advancements, particularly in developing sophisticated game AI. AlphaGO, developed by DeepMind, is a prime example where RL enabled the AI to master the complex game of Go, even defeating world champions. Similarly, RL techniques have been applied to Atari games, allowing AI agents to learn and master these games through trial and error, often achieving superhuman performance.",
            "img": {
              "src": "/assets/images/rl-intro-go-blog-post.jpg",
              "caption": "AlphaGo by Deepmind utilized reinforcement learning methods to beat the top go player Lee Sedol."
            }
          },
          {
            "content": "RL approaches are also trending in robotics and the development of autonomous vehicles. In robotics, RL is used to teach robots how to perform complex tasks such as object manipulation, navigation, and human-robot interaction. For autonomous vehicles, RL helps in decision-making processes, enabling the vehicle to navigate through dynamic environments, avoid obstacles, and optimize routes based on real-time data."
          },
          {
            "content": "Generative AI models, such as those used for creating art, music, or text, benefit from RL by incorporating human feedback to refine their outputs. RL allows these models to adjust their generation strategies based on the rewards or penalties received from human interactions. This approach leads to more nuanced and higher-quality outputs that align better with human preferences and creativity."
          },
          {
            "title": "Future usage of RL",
            "content": "I suspect that RL will become increasingly influential in various industrial sectors over the coming years, particularly in areas such as energy, industrial processes, and manufacturing."
          },
          {
            "content": "In the energy sector, RL can optimize the management of smart grids and renewable energy sources. By learning from real-time data, RL algorithms can enhance energy efficiency, reduce costs, and ensure reliable energy distribution. For instance, RL can dynamically adjust the supply-demand balance in smart grids, optimize the operation of wind and solar power plants, and improve energy storage systems. RL's ability to adapt to real-time changes and optimize decisions in complex environments makes it particularly useful in the energy sector, where conditions and demand can fluctuate rapidly."
          },
          {
            "content": "RL has the potential to transform industrial processes, including carbon capture and utilization. RL algorithms can optimize the parameters of carbon capture systems to maximize efficiency and minimize costs. By learning from operational data, RL can enhance the performance of carbon capture technologies, making them more effective in reducing greenhouse gas emissions. RL's adaptive learning capabilities allow it to handle the complexities and variabilities inherent in distributed industrial processes, leading to more efficient and sustainable operations."
          },
          {
            "content": "In manufacturing, RL can be applied across various stages of the production line, from assembly to maintenance. RL algorithms can help robots learn new tasks, optimize workflows, and ensure precision in repetitive tasks. Predictive maintenance powered by RL can foresee equipment malfunctions, schedule timely interventions, and prevent costly breakdowns. This not only enhances efficiency but also extends the lifespan of machinery. The trial-and-error approach of RL enables continuous improvement and adaptation to changing conditions, making it invaluable in dynamic manufacturing environments."
          },
          {
            "title": "Conclusion",
            "content": "Reinforcement Learning is a powerful machine learning framework that learns and improves over time by interacting with the environment. It has huge potential to solve complex problems and adapt to dynamic situations. Stay tuned for future blog posts where we will delve deeper into the fascinating world of RL!"
          }
        ]
      },
      {
        "title": "En introduksjon til Reinforcement Learning: Hva er det, og hva kan det brukes til?",
        "author": {
          "name": "Thomas Andersen",
          "src": "/assets/images/thomas-2.png"
        },
        "relevantPost": {
          "name": "English version",
          "url": "/blog/1"
        },
        "img": "/assets/images/rl-intro-blog-post.webp",
        "published": "12 min lesing - 5. mai 2024",
        "sections": [
          {
            "content": "Er du nysgjerrig p√• hvordan maskiner kan l√¶re av erfaring og forbedre seg over tid? Hos Digel tror vi at Reinforcement Learning (RL) kan v√¶re et godt rammeverket for √• optimalisere industrielle prosesser. I dette blogginnlegget vil jeg gi deg en enkel introduksjon til RL, hvordan det fungerer, hva som skiller det fra andre maskinl√¶ringsmetoder, og noen bruksomr√•der."
          },
          {
            "title": "De tre hovedtypene maskinl√¶ring",
            "content": "Innenfor maskinl√¶ringsverdenen (ML) kategoriserer vi vanligvis l√¶ringsmetoder i tre klasser: 'Supervised Learning', 'Unsupervised Learning', og RL."
          },
          {
            "content": "I 'Supervised Learning' (veiledet l√¶ring) brukes et datasett som kartlegger innganger til riktige utganger for √• trene AI-agenten. Veiledet l√¶ring brukes ofte til bildeklassifisering, f.eks. klassifisering av dyr i bilder. Datasettet som brukes til √• trene dyreklassifiseringsagenten vil typisk best√• av bilder av et bredt spekter av arter, hver enkelt merket med artsnavnet. Dette er grunnen til at vi kaller det veiledet l√¶ring, siden l√¶ringsprosessen styres av en 'veileder' som gir riktige svar for hvert eksempel i treningsdataene. AI-agenten l√¶rer √• assosiere inngangsbilder med de riktige navnene, og forbedrer sin n√∏yaktighet over tid."
          },
          {
            "content": "P√• den andre siden er 'Unsupervised Learning' (uoverv√•ket l√¶ring) et sett metoder som pr√∏ver √• identifisere m√∏nstre og relasjoner i dataene. En typisk 'unsupervised' l√¶ringstiln√¶rming er klyngingsanalyse, dvs. √• gruppere lignende datapunkter sammen. Hvis vi fortsetter med v√•rt eksempeldatasett best√•ende av dyrearter, kan vi omformulere klassifiseringsproblemet til et uoverv√•ket l√¶ringsproblem hvor vi finner likheter mellom arter fra bildene uten forh√•ndskunnskap om dyrenavnene."
          },
          {
            "content": "Med RL l√¶rer AI-agenten ved √• samhandle med milj√∏et sitt. Agenten mottar tilbakemelding fra milj√∏et i form av bel√∏nninger eller straffer basert p√• sine handlinger. Gjennom pr√∏ving og feiling pr√∏ver agenten √• maksimere sine bel√∏nninger over tid (og minimere straffer) for √• finne den optimale handlingen den b√∏r ta i en gitt tilstand. Hvis vi fortsetter med v√•re dyretematiske eksempler, kan vi pr√∏ve √• trene en RL-agent til √• overleve i naturen. Agenten vil motta en bel√∏nning for hvert m√∏te med et dyr hvor agenten overlever, og vil motta en straff for m√∏ter med rovdyr som spiser agenten. Ved √• simulere nok m√∏ter med forskjellige arter, vil agenten l√¶re √• unng√• et sett med arter.",
            "img": {
              "src": "/assets/images/rl-intro-architecture-blog-post.jpg",
              "caption": "De grunnleggende komponentene i alle 'reinforcement learning'-arkitekturer best√•r av en agent som samhandler med et milj√∏."
            }
          },
          {
            "title": "RL vs. kontrollteori",
            "content": "Som eksemplifisert i forrige avsnitt, trenes RL-agenter til √• ta sekvenser av handlinger som maksimerer en slags kumulativ bel√∏nning. I RL-litteraturen refererer man ofte til beslutningsfunksjonen som agentens 'policy'. M√•let er √• optimalisere 'policyen' p√• en slik m√•te at den maksimerer agentens bel√∏nning n√•r den samhandler med milj√∏et. Med andre ord kan problemene som l√∏ses av RL ogs√• formuleres som kontrollproblemer. 'Policyen' som kontinuerlig oppdateres for hver simuleringssyklus kan betraktes som kontrolloven som brukes for √• ta optimaliserte handlinger i milj√∏et. Hovedforskjellen mellom tradisjonell kontrollteori og RL er at RL inneb√¶rer adaptiv l√¶ring fra interaksjoner, mens kontrollteori vanligvis inneb√¶rer forh√•ndsdesignede kontrollere basert p√• systemmodeller. RL utmerker seg i komplekse, usikre, dynamiske milj√∏er hvor l√¶ring fra erfaring er avgj√∏rende."
          },
          {
            "title": "Hvor brukes RL i dag?",
            "content": "RL brukes for eksempel for √• forbedre forslagssystemer innenfor netthandel. Ved √• analysere brukeradferd og tilbakemeldinger kan RL-algoritmer l√¶re √• foresl√• produkter som er mer sannsynlige √• interessere brukeren, og dermed forbedre brukeropplevelsen og √∏ke salget. For eksempel kan et RL-basert anbefalingssystem foresl√• nye produkter basert p√• tidligere kj√∏p og nettleserhistorikk, og kontinuerlig forbedre sine forslag etter hvert som det l√¶rer mer om brukerens preferanser."
          },
          {
            "content": "I spillindustrien har RL f√∏rt til betydelige fremskritt, spesielt i utviklingen av sofistikert spill-AI. AlphaGO, utviklet av DeepMind, er et godt eksempel hvor RL gjorde det mulig for AI √• mestre det brettspillet Go, og til og med beseire verdensmestere. P√• samme m√•te har RL-teknikker blitt brukt p√• Atari-spill, slik at AI-agenter kan l√¶re og mestre disse spillene gjennom pr√∏ving og feiling, og utklasser ofte de beste menneskelige spillerne.",
            "img": {
              "src": "/assets/images/rl-intro-go-blog-post.jpg",
              "caption": "AlphaGo av Deepmind brukte RL-metoder for √• sl√• den tidligere verdensmesteren i Go, Lee Sedol."
            }
          },
          {
            "content": "RL-tiln√¶rminger er ogs√• trendy innen robotikk og utvikling av autonome kj√∏ret√∏y. I robotikk brukes RL for √• l√¶re roboter √• utf√∏re komplekse oppgaver som manipulering av objekter, navigasjon og menneske-robot-interaksjon. For autonome kj√∏ret√∏y hjelper RL i beslutningsprosesser, slik at kj√∏ret√∏yet kan navigere gjennom dynamiske milj√∏er, unng√• hindringer og optimalisere ruter basert p√• sanntidsdata."
          },
          {
            "content": "Generative AI-modeller, som de som brukes til √• skape kunst, musikk eller tekst, drar nytte av RL ved √• inkludere menneskelig tilbakemelding for √• finjustere sine utganger. RL gj√∏r det mulig for disse modellene √• justere sine generasjonsstrategier basert p√• bel√∏nninger eller straffer mottatt fra menneskelige interaksjoner. Denne tiln√¶rmingen f√∏rer til mer nyanserte og h√∏yere kvalitet utganger som bedre samsvarer med menneskelige preferanser og kreativitet."
          },
          {
            "title": "Fremtidig bruk av RL",
            "content": "Jeg sp√•r at RL vil bli stadig mer innflytelsesrik i ulike industrielle sektorer i l√∏pet av de kommende √•rene, spesielt innen omr√•der som energi, industrielle prosesser og produksjon."
          },
          {
            "content": "I energisektoren kan RL optimalisere styringen av smarte nett og fornybare energikilder. Ved √• l√¶re av sanntidsdata kan RL-algoritmer forbedre energieffektiviteten, redusere kostnader og sikre p√•litelig energidistribusjon. For eksempel kan RL dynamisk justere balansen mellom tilbud og ettersp√∏rsel i smarte nett, optimalisere driften av vind- og solkraftverk, og forbedre energilagringssystemer. RL's evne til √• tilpasse seg sanntidsendringer og optimalisere beslutninger i komplekse milj√∏er gj√∏r det spesielt nyttig i energisektoren, hvor forhold og ettersp√∏rsel kan svinge raskt."
          },
          {
            "content": "RL har potensial til √• transformere styring av industrielle prosesser, for eksempel karbonfangst. RL-algoritmer kan optimalisere parameterne til karbonfangstsystemer for √• maksimere CO2-fangst og minimere kostnader. Ved √• l√¶re av operasjonelle data kan RL forbedre ytelsen til karbonfangstteknologier, gj√∏re dem mer effektive i √• redusere klimagassutslipp. RLs adaptive l√¶ringsevner gj√∏r det i stand til √• h√•ndtere dynamikken i distribuerte industrielle prosesser, noe som f√∏rer til mer effektive og b√¶rekraftige operasjoner."
          },
          {
            "content": "I produksjon kan RL brukes p√• tvers av ulike stadier av produksjonslinjen, fra montering til vedlikehold. RL-algoritmer kan hjelpe roboter med √• l√¶re nye oppgaver, optimalisere arbeidsflyter og sikre presisjon i repeterende oppgaver. Prediktivt vedlikehold drevet av RL kan forutse utstyrsfeil, planlegge tidsriktige inngrep og forhindre kostbare sammenbrudd. Dette forbedrer ikke bare effektiviteten, men forlenger ogs√• levetiden til maskineriet. Pr√∏ving-og-feiling-tiln√¶rmingen til RL muliggj√∏r kontinuerlig forbedring og tilpasning til endrede forhold, noe som gj√∏r det uvurderlig i dynamiske produksjonsmilj√∏er."
          },
          {
            "title": "Konklusjon",
            "content": "Reinforcement Learning er et krafig rammeverk innenfor maskinl√¶ring som l√¶rer og forbedrer seg over tid ved √• samhandle med milj√∏et. Det har stort potensial til √• l√∏se komplekse problemer og tilpasse seg dynamiske situasjoner. F√∏lg med p√• fremtidige blogginnlegg hvor vi vil dykke dypere inn i den fascinerende RL-verdenen!"
          }
        ]
      }
    ]
  },
  "about": {
    "sections": [
      {
        "name": "E-mail",
        "content": "hello@digel.io"
      },
      {
        "name": "Phone number",
        "content": "+47 95744191"
      },
      {
        "name": "Organization number",
        "content": "933 191 656"
      }
    ],
    "socialMedia": {
      "github": "https://github.com/digel-as",
      "linkedin": "https://www.linkedin.com/company/102369767"
    }
  }
}
